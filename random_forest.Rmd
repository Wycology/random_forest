---
title: "Using `randomForest()` in R"
author: "Wyclife Agumba Oluoch"
date: "`r Sys.time()`"
bibliography:
  - bib/packages.bib
nocite: '@*'
output: 
    html_document:
      toc: true
      toc_depth: 2
      toc_float: true
      theme: darkly
      #highlight: pygments
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libs, echo=FALSE, warning=FALSE, message=FALSE, include=FALSE}
packages <- c("base",
              'knitr',
              'rmarkdown',
              'prettydoc',
              'randomForest',
              'datasets',
              'caret')
installed_packages <- packages %in% rownames(installed.packages())
if(any(installed_packages == FALSE)){
  install.packages(packages[!installed_packages])
}
lapply(packages, library, character.only = TRUE) |> invisible()
```

```{r write_bib, echo=FALSE, warning=FALSE, message=FALSE, include=FALSE}
knitr::write_bib(c(
  .packages(), packages
), 'bib/packages.bib')
```

# Building `randomForest()` model in `R`

This work was developed following this [blog post](https://www.r-bloggers.com/2021/04/random-forest-in-r/#:~:text=Random%20Forest%20in%20R%2C%20Random,to%20identify%20the%20important%20attributes).

I show how to use `randomForest()` in `R` to train a model and make prediction on a new dataset. **New data** here actually has all features that were used in the training data. Generally, the two sets should be overlapping in values, though not exactly the same. By this I mean, it does not make sense to train a model then use it to make prediction on data with ranges beyond that of the training set.

We will use the in-built iris data for this demonstration. In that case, we use the four continuous variables (**`r names(iris[-5])`**) to predict **`r names(iris[5])`** of iris flower.

## Loading the data

```{r data_loading}
data <- iris
str(data) # Check the existing variables and their corresponding types.
```

## Partitioning the data

Here, we break down the data into two separate sets with one part to be used for training/calibrating/building the model and the other part being left for testing the model performance once it is built successfully. Normally, training set should be having more data than the test set, especially when there is limited records of data. But can vary depending on specific use case.

```{r}
set.seed(1985) # To enable reproducibility
ind <- sample(x = 2, size = nrow(data), replace = T, prob = c(0.7, 0.3))
train <- data[ind == 1,] # Retains the rows of data where index == 1 is TRUE
test <- data[ind == 2,] # Retains the rows of data where index == 2 is TRUE
```

## Building the `randomForest()` model

Building the random forest model using the target variable and all the continuous variables within the train data-set.

```{r building_model}
rf <- randomForest(Species ~., data = train, proximity = TRUE)
print(rf)
```
## Making prediction

Predicting on the data-set used in training the model.

```{r}
predict_train <- predict(rf, train) # Predict on train data-set 
confusionMatrix(predict_train, train$Species)
```

Makingredicting on test data-set

```{r}
predict_test <- predict(rf, test)
confusionMatrix(predict_test, test$Species)
```

## Model performance

Error rate of the model.
 
```{r}
plot(rf, main = "Plot of the randomForest model")
```

## Variable importance

Kind of showing which were the most important variables in predicting species types.

```{r}
varImp(rf)
```

So according to this, it seems `Petal.Length` and `Petal.Width` are the top two important variables in predicting **Species** of iris. Let us plot these variables individually.

```{r}
plot(iris$Petal.Length, iris$Petal.Width, col = iris$Species)
plot(iris$Petal.Length, col = iris$Species)
plot(iris$Petal.Width, col = iris$Species)
```

Seems we can see the species grouping themselves distinctively using the two variables.

Tuning the model, although it already performed well.

```{r}
t <- tuneRF(train[, -5], train[, 5],
            stepFactor = 0.5,
            plot = TRUE,
            ntreeTry = 150,
            trace = TRUE,
            improve = 0.05)
```


```{r}
hist(treesize(rf),
     main = 'No. of nodes for the trees',
     col = 'green')

varImpPlot(rf, 
           sort = TRUE,
           n.var = 10,
           main = 'Top 10 variable importance')

importance(rf)
```

Partial dependence plot

```{r}
partialPlot(rf, train, Petal.Width, 'setosa')
```

Multi-dimensional plot

```{r}
MDSplot(rf, train$Species, pch = 10)
```
